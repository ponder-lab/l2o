import tensorflow as tf
import time


def weights_sum(n):
    return tf.ones([n])


def weights_mean(n):
    return tf.ones([n]) / tf.cast(n, tf.float32)


class MetaOptimizerMgr:

    def __init__(
            self, optimizer, learner,
            noise_stddev=0.0, unroll=20, unroll_weights=weights_mean):

        self.optimizer = optimizer
        self.learner = learner

        self.noise_stddev = noise_stddev
        self.unroll = unroll
        self.unroll_weights = unroll_weights

    def _make_cf(
            self, weights, data, problem, params=None, states=None,
            is_batched=False, teacher=None):
        """Helper function to make a concrete function for the given params.

        A concrete function is a single graph generated by AutoGraph; see
        ``https://www.tensorflow.org/guide/concrete_function``.

        In general, the rules are (as of 2.3.0-rc0):
          - Nested structures (lists, dicts of Tensors) must maintain the same
            internal values and dimensions
          - Python objects are 'bound' and must be constant (i.e. id())
          - BUG: non-primitive python objects can only be passed during
            .get_concrete_function() and must not be passed again when called.
            This is because non-primitive objects are interpreted as
            ``UnknownArgument`` by tensorflow.
        """

        kwargs = dict(
            params=params, states=states, unroll=self.unroll, problem=problem,
            is_batched=is_batched)
        if teacher is None:
            return self.learner.meta_loss.get_concrete_function(
                weights, data, noise_stddev=self.noise_stddev, **kwargs)
        else:
            return self.learner.imitation_loss.get_concrete_function(
                weights, data, teacher=teacher, **kwargs)

    def _train_full(self, problem, teacher=None, repeat=1):
        """Full batch training.

        Parameters
        ----------
        problem : problem.Problem
            Problem to train on. Presence of ``.get_dataset()`` or
            ``.get_internal()`` indicates full or minibatch.

        Keyword Args
        ------------
        teacher : tf.keras.optimizers.Optimizer
            Optimizer to imitate; trains meta-loss if not passed.
        repeat : int
            Number of times to repeat. reset() is used for computational
            efficiency (the loss graph is not rebuilt between repeats).
        """

        pbar = tf.keras.utils.Progbar(repeat, unit_name='step')
        concrete_loss = None

        for _ in range(repeat):

            weights = self.unroll_weights(self.unroll)
            data = problem.get_internal()

            if teacher is not None:
                problem.reset()

            if concrete_loss is None:
                concrete_loss = self._make_cf(
                    weights, data, problem, teacher=teacher)

            trainable = self.learner.trainable_variables
            with tf.GradientTape(watch_accessed_variables=False) as tape:
                tape.watch(trainable)
                loss, _, _ = concrete_loss(weights, data)
            grads = tape.gradient(loss, trainable)
            self.optimizer.apply_gradients(zip(grads, trainable))

            pbar.add(1, values=[("loss", loss)])

    def _train_batch(self, problem, teacher=None, epochs=1):
        """Minibatch training.

        Parameters
        ----------
        problem : problem.Problem
            Problem to train on. Presence of ``.get_dataset()`` or
            ``.get_internal()`` indicates full or minibatch.

        Keyword Args
        ------------
        teacher : tf.keras.optimizers.Optimizer
            Optimizer to imitate; trains meta-loss if not passed.
        epochs : int
            Number of epochs to run for.
        """

        concrete_loss = None

        params = problem.get_parameters()
        states = [self.learner._initialize_state(p) for p in params]

        for i in range(epochs):

            print("Epoch {}".format(i + 1))
            pbar = tf.keras.utils.Progbar(
                problem.size(self.unroll), unit_name='step')

            dataset = problem.get_dataset(self.unroll)
            weights = self.unroll_weights(self.unroll)

            for batch in dataset:

                if teacher is not None:
                    teacher.reset(values=params)

                batch_stacked = [
                    tf.stack(tf.split(dim, num_or_size_splits=self.unroll))
                    for dim in batch
                ]

                if concrete_loss is None:
                    concrete_loss = self._make_cf(
                        weights, batch_stacked, problem,
                        params=params, states=states, is_batched=True,
                        teacher=teacher)

                trainable = self.learner.trainable_variables
                with tf.GradientTape(watch_accessed_variables=False) as tape:
                    tape.watch(trainable)
                    loss, params, states = concrete_loss(
                        weights, batch_stacked, params=params, states=states)
                grads = tape.gradient(loss, trainable)
                self.optimizer.apply_gradients(zip(grads, trainable))

                pbar.add(1, values=[("loss", loss)])

    def train(self, problems, epochs=1, repeat=1, teacher=None):
        """Run meta-training.

        Parameters
        ----------
        problems : problem.ProblemSpec[]
            List of problem specifications to build and run

        Keyword Args
        ------------
        epochs : int
            Number of epochs to run if batched
        repeat : int
            Number of repetitions to run using the same graph if full batched.
        teacher : tf.keras.optimizers.Optimizer
            If passed, runs imitation learning instead against ``teacher``.
        """

        start = time.time()
        for itr, spec in enumerate(problems):
            spec.print(itr)
            problem = spec.build(persistent=teacher is not None)

            if hasattr(problem, "get_dataset"):
                self._train_batch(problem, teacher=teacher, epochs=epochs)
            else:
                self._train_full(problem, teacher=teacher, repeat=repeat)

        return time.time() - start
