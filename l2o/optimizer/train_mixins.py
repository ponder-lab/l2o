import tensorflow as tf
import collections

from tensorflow.keras.utils import Progbar


_MetaIteration = collections.namedtuple(
    "MetaIteration",
    ["problem", "optimizer", "unroll_len", "unroll_weights", "teacher"])


class MetaIteration(_MetaIteration):
    """Meta Iteration args storage class

    Attributes
    ----------
    problem : problem.Problem
        Problem to train on. Presence of ``.get_dataset()`` or
        ``.get_internal()`` indicates full or minibatch.
    optimizer : tf.keras.optimizers.Optimizer
        Optimizer to use for meta optimization
    unroll_len : Callable -> int
        Callable that returns unroll size.
    unroll_weights : Callable(int) -> tf.Tensor
        Callable that generates unroll weights from an unroll size.
    teacher : tf.keras.optimizers.Optimizer
        Optimizer to imitate; trains meta-loss if not passed.
    """
    pass


def weights_sum(n):
    return tf.ones([n])


def weights_mean(n):
    return tf.ones([n]) / tf.cast(n, tf.float32)


class TrainingMixin:

    def _make_cf(
            self, meta, weights, data, unroll, params=None, states=None,
            global_state=None, is_batched=False):
        """Helper function to make a concrete function for the given params.

        A concrete function is a single graph generated by AutoGraph; see
        ``https://www.tensorflow.org/guide/concrete_function``.

        In general, the rules are (as of 2.3.0-rc1):
          - Nested structures (lists, dicts of Tensors) must maintain the same
            internal values and dimensions
          - Python objects are 'bound' and must be constant (i.e. id())
          - BUG: non-primitive python objects can only be passed during
            .get_concrete_function() and must not be passed again when called.
            This is because non-primitive objects are interpreted as
            ``UnknownArgument`` by tensorflow.
        """

        kwargs = dict(
            params=params, states=states, global_state=global_state,
            unroll=unroll, problem=meta.problem, is_batched=is_batched)
        if meta.teacher is None:
            return self.meta_loss.get_concrete_function(
                weights, data,
                noise_stddev=meta.problem.noise_stddev, **kwargs)
        else:
            return self.imitation_loss.get_concrete_function(
                weights, data, teacher=meta.teacher, **kwargs)

    def _step(
            self, optimizer, concrete_loss, weights, data,
            params=None, states=None):
        """Helper function to run for a single step."""

        # Specify trainable_variables specifically for efficiency
        with tf.GradientTape(watch_accessed_variables=False) as tape:
            tape.watch(self.trainable_variables)
            # Other arugments of ``concrete_loss`` are bound and do not
            # need to be passed.
            loss, params, states = concrete_loss(
                weights, data, params=params, states=states)
        # Standard apply_gradient paradigam
        # Used instead of ``optimizer.minimize`` to expose the current loss
        grads = tape.gradient(loss, self.trainable_variables)
        optimizer.apply_gradients(zip(grads, self.trainable_variables))

        return loss, params, states

    def _train_full(self, meta, repeat=1):
        """Full batch training.

        Parameters
        ----------
        meta : MetaIteration
            Current metaiteration parameters. See docstring.

        Keyword Args
        ------------
        repeat : int
            Number of times to repeat. reset() is used for computational
            efficiency (the loss graph is not rebuilt between repeats).
        """

        pbar = Progbar(repeat, unit_name='step')
        concrete_loss = None

        for _ in range(repeat):

            unroll = meta.unroll_len()
            weights = meta.unroll_weights(unroll)
            data = meta.problem.get_internal()

            # Reset only required when ``persistent=True``
            if meta.teacher is not None:
                meta.problem.reset()

            # Only create concrete loss on first iteration
            if concrete_loss is None:
                concrete_loss = self._make_cf(meta, weights, data, unroll)

            loss, _, _, _ = self._step(
                meta.optimizer, concrete_loss, weights, data)

            pbar.add(1, values=[("loss", loss)])

    def _train_batch(self, meta, epochs=1):
        """Minibatch training.

        Parameters
        ----------
        meta : MetaIteration
            Current metaiteration parameters. See docstring.

        Keyword Args
        ------------
        epochs : int
            Number of epochs to run for.
        """

        concrete_loss = None

        # Batch training stores persistent state
        params, states, global_state = self._get_state(
            meta.problem, params=None, states=None, global_state=None)

        for i in range(epochs):

            unroll = meta.unroll_len()
            weights = meta.unroll_weights(unroll)
            dataset = meta.problem.get_dataset(unroll)

            print("Epoch {}".format(i + 1))
            pbar = Progbar(meta.problem.size(unroll), unit_name='step')

            for batch in dataset:

                if meta.teacher is not None:
                    meta.problem.reset(values=params)

                # Data dimensions are ``[unroll, batch] + [data]``
                batch_stacked = [
                    tf.stack(tf.split(dim, num_or_size_splits=unroll))
                    for dim in batch]

                if concrete_loss is None:
                    concrete_loss = self._make_cf(
                        meta, weights, batch_stacked, unroll, params=params,
                        states=states, global_state=global_state,
                        is_batched=True)

                loss, params, states, global_state = self._step(
                    meta.optimizer, concrete_loss, weights, batch_stacked,
                    params=params, states=states, global_state=global_state)

                pbar.add(1, values=[("loss", loss)])

    def train(
            self, problems, optimizer, unroll_len=lambda: 20,
            unroll_weights=weights_mean, teacher=None, epochs=1, repeat=1):
        """Run meta-training.

        Parameters
        ----------
        problems : problem.ProblemSpec[]
            List of problem specifications to build and run
        optimizer : tf.keras.optimizers.Optimizer
            Optimizer to use for meta optimization

        Keyword Args
        ------------
        unroll_len : Callable -> int
            Unroll size or callable that returns unroll size.
        unroll_weights : Callable(int) -> tf.Tensor
            Callable that generates unroll weights from an unroll size.
        epochs : int
            Number of epochs to run if batched
        repeat : int
            Number of repetitions to run using the same graph if full batched.
        teacher : tf.keras.optimizers.Optimizer
            If passed, runs imitation learning instead against ``teacher``.
        """

        for itr, spec in enumerate(problems):
            spec.print(itr)
            problem = spec.build(persistent=teacher is not None)

            meta = MetaIteration(
                problem, optimizer, unroll_len, unroll_weights, teacher)

            if hasattr(problem, "get_dataset"):
                self._train_batch(meta, epochs=epochs)
            elif hasattr(problem, "get_internal"):
                self._train_full(meta, repeat=repeat)
            else:
                raise TypeError(
                    "Problem must be able to either get_dataset() or"
                    + "get_internal().")
