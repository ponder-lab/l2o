"""Outer Optimization Step."""
import tensorflow as tf


class StepMixin:
    """Outer Optimization Step Mixin."""

    def _base_step(self, callable):
        """Run a single step.

        In addition to the standard GradientTape -> gradient -> apply_gradients
        paradigam, additional logic is included to detect 'first time'
        execution of the keras model ``self.network``.

        This is because ``trainable_variables`` is not defined beforehand,
        so ``watch_accessed_variables`` needs to be set to True in order to
        capture them as they are created. However, this is still not the
        default in order to maintain efficiency by ignoring non-trainable
        variables.
        """

        # trainable_variables not yet built -> capture all variables
        if len(self.network.trainable_variables) <= 2:
            with tf.GradientTape() as tape:
                results = callable()
        # trainable_variables built -> capture only learner variables
        else:
            with tf.GradientTape(watch_accessed_variables=False) as tape:
                tape.watch(self.network.trainable_variables)
                results = callable()

        grads = tape.gradient(results[0], self.network.trainable_variables)
        self.optimizer.apply_gradients(
            zip(grads, self.network.trainable_variables))

        return results

    @tf.function
    def abstract_step(self, data, params, opt=None, **kwargs):
        """Wraps imitation_loss to compute meta-gradients inside graph mode.

        See ``abstract_loss`` for docstring and ``_base_step`` for internal
        mechanism.
        """

        def _tmp(data, params):
            # trainable_variables not yet built -> capture all variables
            # if len(self.network.trainable_variables) <= 2:
            with tf.GradientTape() as tape:
                results = self.abstract_loss(data, params, **kwargs)
            # trainable_variables built -> capture only learner variables
            # else:
            #     with tf.GradientTape(watch_accessed_variables=False) as tape:
            #         tape.watch(self.network.trainable_variables)
            #         results = self.abstract_loss(data, params, **kwargs)

            grads = tape.gradient(results[0], self.network.trainable_variables)
            return grads, results

        grads, results = self.distribute.run(_tmp, args=(data, params))

        self.optimizer.apply_gradients(
            zip(grads, self.network.trainable_variables))

        return results

    def make_concrete_step(self, meta, data, params):
        """Get a concrete @tf.function instance for abstract_step.

        A concrete function is a single graph generated by AutoGraph; see
        ``https://www.tensorflow.org/guide/concrete_function``.

        In general, the rules are (as of 2.3.0-rc1):
          - Nested structures (lists, dicts of Tensors) must maintain the same
            internal values and dimensions
          - Python objects are 'bound' and must be constant (i.e. id())
          - BUG: non-primitive python objects can only be passed during
            .get_concrete_function() and must not be passed again when called.
            This is because non-primitive objects are interpreted as
            ``UnknownArgument`` by tensorflow.

        Parameters
        ----------
        meta : MetaIteration
            Namedtuple containing problem parameters.
        data : nested structure
            Sample data element for concrete function binding.
        params : tf.Tensor[]
            Initial problem parameter values.

        Returns
        -------
        tf.Graph
            Concrete function created with the specified problem inputs.
        """
        # Weights are just placeholders
        kwargs = dict(
            unroll=meta.unroll_len,
            problem=meta.problem,
            seed=meta.seed,
            meta_loss_weight=tf.constant(0.5),
            imitation_loss_weight=tf.constant(0.5))
        args = (data, params)

        if meta.validation:
            return self.abstract_loss.get_concrete_function(*args, **kwargs)
        else:
            return self.abstract_step.get_concrete_function(*args, **kwargs)
